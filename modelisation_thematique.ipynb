{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this Python notebook, we are doing topic modeling based on available Python packages in this research field.\n",
    "\n",
    "## Required Software\n",
    "* Java (JDK >= 16): [Java SE Development Kit](https://www.oracle.com/java/technologies/downloads/)\n",
    "* Apache Ant (version >= 1.10.10): [supplies a number of built-in tasks allowing to compile, assemble, test and run Java applications.](https://ant.apache.org/bindownload.cgi)\n",
    "* MALLET 2.0.8: [MAchine Learning for LanguagE Toolkit](https://mallet.cs.umass.edu/download.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python packages\n",
    "For this notebook, required Python packages are:\n",
    "* `nltk`: the [Natural Language Toolkit](https://www.nltk.org/)\n",
    "* `gensim/3.8.3`: the [\"*fastest library for training of vector embeddings*\"](https://radimrehurek.com/gensim_3.8.3/)\n",
    "* `spacy`: [Industrial-Strength Natural Language Processing](https://spacy.io/)\n",
    "* `pyLDAvis/3.3.1`: [Python library for interactive topic model visualization](https://pypi.org/project/pyLDAvis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you need to install the above packages in your environment:\n",
    "# !pip install matplotlib numpy pandas click==7.1.2\n",
    "# !pip install nltk gensim==3.8.3 spacy pyLDAvis==3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also required - the spaCy English vocabulary\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard and scientific packages | Modules réguliers et scientifiques\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# NLTK - Natural Language Toolkit\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Only required on the first execution\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "\n",
    "# spaCy for lemmatization | spaCy pour lemmatisation\n",
    "import spacy\n",
    "\n",
    "# Plotting tools | outils graphiques\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable logging for gensim - optional | activé le registre pour gensim - en option\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "* Load stop words from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words | NLTK Mots vides\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words = stopwords.words('french')\n",
    "\n",
    "# See the default list | voir la liste pas défaut\n",
    "print('Default list:', stop_words)\n",
    "\n",
    "# Add your custom stop words | ajoutez vos mots vides personnalisés \n",
    "stop_words.extend([])\n",
    "\n",
    "# See the final list of stop words | voir la liste complète\n",
    "print('\\nFinal list:', stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the list of filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path and extension of text files\n",
    "txt_folder = Path('data/').rglob('*.txt')\n",
    "#collecter les chemins de fichiers pour tous vos fichiers texte\n",
    "#txt_folder = Path('donnee/').rglob('*.txt')\n",
    "\n",
    "files = [x for x in txt_folder]  # Gather the paths for all text files in a list\n",
    "print(files[:3], '...', files[-3:])  # Print first 3 and last 3 filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a dictionary that will populate a Pandas DataFrame with two columns:\n",
    "  * `target_names`: the filename without its path\n",
    "  * `content`: the original text data of the file in single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#créer un dictionnaire qui contient tous les noms de fichiers\n",
    "#et les associe à leur texte\n",
    "papers = {'target_names': [], 'content': []}\n",
    "\n",
    "for name in files:\n",
    "    f = open(name, 'r', encoding='utf-8')\n",
    "    basename = os.path.basename(name)\n",
    "\n",
    "    # Print at every 10 filenames\n",
    "    if name in files[::10]:\n",
    "        print(f'Reading {basename} ...')\n",
    "\n",
    "    papers['target_names'].append(basename)\n",
    "    papers['content'].append(' '.join(f.readlines()))\n",
    "    f.close()\n",
    "\n",
    "# Convert the dictionary to a pandas data frame \n",
    "# convertir le dictionnaire en dataframe pandas\n",
    "df = pd.DataFrame.from_dict(papers)\n",
    "print(f'Total: {len(df)} rows. Here are the first five:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text content to a list\n",
    "# Convertir le contenu du texte en liste\n",
    "data = papers['content']\n",
    "\n",
    "# Remove roman numerals | Supprimer les chiffres romains\n",
    "data = [re.sub('[MDCLXVI]+(\\.|\\b\\w\\n)', ' ', sentence) for sentence in data]\n",
    "\n",
    "# Remove new line characters | Supprimer les caractères de nouvelle ligne\n",
    "data = [re.sub('\\s+', ' ', sentence) for sentence in data]\n",
    "\n",
    "# Remove distracting quotes | Supprimer les citations distrayantes\n",
    "#data = [re.sub(\"\\'\", \"\", sentence) for sentence in data]\n",
    "\n",
    "print('First cleaned sentence:', data[0])\n",
    "print('\\nLast cleaned sentence:', data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supprimer la ponctuation et collecter tous les mots individuels\n",
    "def sentences_to_words(sentences):\n",
    "    \"\"\"\n",
    "    Generator - For each sentence, return a processed list of words\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    Each sentence processed by gensim.utils.simple_preprocess(), which\n",
    "    removes the punctuation and collects all the individual words.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        # Setting deacc=True removes punctuations\n",
    "        yield(simple_preprocess(sentence, deacc=True))\n",
    "\n",
    "# Create a list of lists of words - one list of words per sentence\n",
    "data_words = list(sentences_to_words(data))\n",
    "\n",
    "print('First list of words:', data_words[0])\n",
    "print('\\nLast list of words:', data_words[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "We will start by using:\n",
    "* Gensim's [Phrases class](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases) - an instance of it \"detects phrases based on collocation counts\"\n",
    "* Gensim's [Phraser class](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phraser) - an alias of [FrozenPhrases](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.FrozenPhrases) which \"cuts down memory consumption of Phrases, by discarding model state not strictly needed for the phrase detection task\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models - higher threshold => fewer phrases\n",
    "#Construire les modèles bigramme et trigramme\n",
    "bigram = gensim.models.phrases.Phrases(data_words, min_count=4, threshold=8)\n",
    "trigram = gensim.models.phrases.Phrases(bigram[data_words], threshold=8)\n",
    "\n",
    "# Faster way to get a sentence identified as a trigram/bigram\n",
    "## Moyen plus rapide d'obtenir une phrase identifiée comme un trigramme/bigramme\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See a trigram example | voir l'exemple trigramme\n",
    "print(trigram_mod[bigram_mod[data_words[90]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "## Définir des fonctions pour les mots vides, les bigrammes, les trigrammes et la lemmatisation\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words | Supprimer les mots vides\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams | faire les bigrammes\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Form trigrams | faire les trigrammes\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Initialiser le modèle spacy 'fr', en ne gardant que le composant tagger (pour plus d'efficacité)\n",
    "#nlp = spacy.load('fr_core_news_sm', disable=['parser', 'ner'])\n",
    "#nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "# Faire la lemmatisation en ne gardant que le nom, l'adj, le vb, l'adv\n",
    "data_lemmatized = lemmatization(data_words_trigrams,\n",
    "                                allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary | créer le dictionnaire\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Term Document Frequency | Durée Document Fréquence\n",
    "corpus = [id2word.doc2bow(text) for text in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readable format of corpus | format lisible du corpus\n",
    "[[(id2word[id], freq) for id, freq in cp[:10]] for cp in corpus[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2   # Set the minium number of topics your model will run | choisissez le nombre minimum de thème\n",
    "limit = 11  #choose the max ceiling for number of topics, your model will have a max of one less than this ceiling \n",
    "#choisissez le nombre maximum plafond de thème, votre modèle aura un thème de moins \n",
    "step = 2    # Set the step width for number of topics per model | choisissez la taille du pas\n",
    "mallet_path = '~/mallet-2.0.8/bin/mallet' # update this path to the path to your mallet program\n",
    "\n",
    "model_list = []\n",
    "coherence_values = []\n",
    "\n",
    "for num_topics in range(start, limit, step):\n",
    "    model = gensim.models.wrappers.LdaMallet(\n",
    "        mallet_path=mallet_path,\n",
    "        corpus=corpus,\n",
    "        num_topics=num_topics,\n",
    "        id2word=id2word)\n",
    "    model_list.append(model)\n",
    "\n",
    "    coherencemodel = CoherenceModel(\n",
    "        model=model,\n",
    "        texts=data_lemmatized,\n",
    "        dictionary=id2word,\n",
    "        coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph | voir le graphique\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores | voir les cohérences\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics|Numero de Théme =\", m,\n",
    "          \" has Coherence Value of|a une cohérence de\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "#choissisez le mieux modéle et voir les thémes\n",
    "# Choose which model in the list you think is the best\n",
    "# Remember python started indexing from 0\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run just that model with the exact number of topics you want\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(\n",
    "    mallet_path, corpus=corpus, num_topics=8, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Topics | voir les thèmes\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# see the Coherence Score | voir la cohérance\n",
    "coherence_model_ldamallet = CoherenceModel(\n",
    "    model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "#visualiser les thèmes\n",
    "# Can't use the gensim method for MALLET directly\n",
    "# So converting LdaMallet Model to LdaModel as per\n",
    "# https://radimrehurek.com/gensim/models/wrappers/ldamallet.html\n",
    "# Note that a \"by hand\" version of doing thing can be found at\n",
    "# https://jeriwieringa.com/2018/07/17/pyLDAviz-and-Mallet/\n",
    "\n",
    "lda_model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(\n",
    "    ldamallet, gamma_threshold=0.01, iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The notebook crashes because of this cell\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=df):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                    pd.Series(\n",
    "                        [int(topic_num), round(prop_topic,4), topic_keywords]),\n",
    "                    ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    sent_topics_df.columns = [\n",
    "        'Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = texts\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(\n",
    "    ldamodel=ldamallet, corpus=corpus, texts=df)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "    'Document number',\n",
    "    'Dominant_Topic',\n",
    "    'Topic_Perc_Contrib',\n",
    "    'Keywords',\n",
    "    'file_name',\n",
    "    'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 Kernel",
   "language": "python",
   "name": "20220211_1755"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
