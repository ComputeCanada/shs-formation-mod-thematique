{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation thématique\n",
    "Dans ce notebook, nous effectuons de la modélisation thématique de textes à l'aide de modules Python spécialisés.\n",
    "\n",
    "**IMPORTANT**: ce notebook requiert le module `java`. S'il n'était pas chargé au moment d'ouvrir ce notebook, vous devez le fermer, l'arrêter, charger le module `java` et rouvrir le présent notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des modules Python requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules réguliers et scientifiques\n",
    "print('- Chargement des modules réguliers...')\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# NLTK - Natural Language Toolkit\n",
    "print('- Chargement de NLTK...')\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Requis seulement une fois\n",
    "\n",
    "# Gensim\n",
    "print('- Chargement de Gensim...')\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "\n",
    "# spaCy pour la lemmatisation\n",
    "print('- Chargement de spaCy...')\n",
    "import spacy\n",
    "\n",
    "# Outils de visualisation\n",
    "print('- Chargement des outils de visualisation...')\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurer la journalisation de Gensim (optionnel)\n",
    "print('- Configuration finale...')\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "    level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "print('Chargement des modules terminé.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "* Charger les \"mots vides\" de la langue française à partir du module NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mots vides dans NLTK sont les \"stopwords\"\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('french')\n",
    "\n",
    "# Afficher la liste par défaut\n",
    "print('Liste par défaut:\\n', stop_words)\n",
    "\n",
    "# Ajouter d'autres mots à la liste, au besoin\n",
    "stop_words.extend([])\n",
    "\n",
    "# Afficher la liste finale\n",
    "print('\\nListe finale:\\n', stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtenir la liste des fichiers texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir le chemin vers tous les fichiers texte dans le dossier \"donnees/\"\n",
    "txt_folder = Path('donnees/').rglob('*.txt')\n",
    "\n",
    "files = sorted([x for x in txt_folder])  # Convertir le tout en une liste triée\n",
    "print(files[:3], '...', files[-3:])  # Afficher les premiers et derniers fichiers\n",
    "print(f' => {len(files)} fichiers au total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Créer un dictionnaire qui servira à initialiser un DataFrame Pandas avec deux colonnes :\n",
    "  * `target_names`: le nom du fichier et son chemin\n",
    "  * `content`: le texte original du fichier regroupé en une seule ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {'target_names': [], 'content': []}\n",
    "\n",
    "# Pour chaque fichier texte\n",
    "for name in files:\n",
    "    f = open(name, 'r', encoding='utf-8')\n",
    "    basename = os.path.basename(name)\n",
    "\n",
    "    # Afficher la progression à tous les 10 fichiers\n",
    "    if name in files[::10]:\n",
    "        print(f'Reading {basename} ...')\n",
    "\n",
    "    # Noter le nom du fichier et son contenu\n",
    "    text_dict['target_names'].append(basename)\n",
    "    text_dict['content'].append(' '.join(f.readlines()))\n",
    "    f.close()\n",
    "\n",
    "# Convertir le dictionnaire en dataframe pandas\n",
    "df = pd.DataFrame.from_dict(text_dict)\n",
    "print(f'Total: {len(df)} rangées. Voici les 5 premières:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyer les données textuelles\n",
    "* Enlever les chiffres romains et les espaces multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner le contenu de tous les fichiers\n",
    "data = text_dict['content']\n",
    "\n",
    "# Supprimer les chiffres romains\n",
    "data = [re.sub('[MDCLXVI]+(\\.|\\b\\w\\n)', ' ', sentence) for sentence in data]\n",
    "\n",
    "# Remplacer les espaces (et sauts de ligne) multiples par un simple espace\n",
    "data = [re.sub('\\s+', ' ', sentence) for sentence in data]\n",
    "\n",
    "# Supprimer les caractères de citations\n",
    "#data = [re.sub(\"\\'\", \"\", sentence) for sentence in data]\n",
    "\n",
    "print(f'Premier texte nettoyé:\\n {data[0][:308]}...\\n')\n",
    "print(f'Dernier texte nettoyé:\\n {data[-1][:308]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Enlever tous les symboles de ponctuation et transformer chaque texte en liste de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_words(sentences):\n",
    "    \"\"\"\n",
    "    Générateur - Pour chaque texte, retourner une liste de mots\n",
    "\n",
    "    Retourne:\n",
    "    ---------\n",
    "    Chaque texte est traité par gensim.utils.simple_preprocess() qui\n",
    "    enlève la ponctuation et collecte tous les mots individuels.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        # L'option deacc=True enlève les symboles de ponctuation\n",
    "        yield(simple_preprocess(sentence, deacc=True))\n",
    "\n",
    "# Créer une liste de listes de mots - une liste de mots par texte\n",
    "data_words = list(sentences_to_words(data))\n",
    "\n",
    "print('Première liste de mots:\\n', data_words[0][:50], '...\\n')\n",
    "print('Dernière liste de mots:\\n', data_words[-1][:50], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation thématique\n",
    "On commence par utiliser:\n",
    "* [la classe Phrases](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phrases) de Gensim - détecte les phrases en fonction des décomptes de collocation\n",
    "* [la classe Phraser](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.Phraser) (alias de [FrozenPhrases](https://radimrehurek.com/gensim/models/phrases.html#gensim.models.phrases.FrozenPhrases)) de Gensim - réduit la consommation de mémoire-vive en éliminant les informations optionnelles pour la détection de phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construire les modèles bigramme et trigramme - threshold élevé => moins de phrases\n",
    "bigram = gensim.models.phrases.Phrases(data_words, min_count=4, threshold=8)\n",
    "trigram = gensim.models.phrases.Phrases(bigram[data_words], threshold=8)\n",
    "\n",
    "# Moyen plus rapide d'obtenir une phrase identifiée comme un trigramme/bigramme\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Voir l'exemple d'un trigramme\n",
    "for mot in trigram_mod[bigram_mod[data_words[0]]]:\n",
    "    if len(mot.split('_')) == 3:\n",
    "        print(mot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Définir des fonctions pour traiter les mots vides, les bigrammes, les trigrammes et la lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [\n",
    "        [word for word in simple_preprocess(str(doc)) if word not in stop_words]\n",
    "        for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compléter le nettoyage des listes de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('- Supprimer les mots vides...')\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "print('- Former les bigrammes...')\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "print('- Former les trigrammes...')\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)\n",
    "\n",
    "# Initialiser le modèle spaCy 'fr', en ne gardant que le composant \"tagger\"\n",
    "print('- Initialiser le modèle spaCy...')\n",
    "nlp = spacy.load('fr_core_news_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Faire la lemmatisation en ne gardant que les noms, adjectifs, verbes et adverbes\n",
    "print('- Lemmatisation...')\n",
    "data_lemmatized = lemmatization(data_words_trigrams,\n",
    "                                allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Création du dictionnaire et du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le dictionnaire\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Calculer la fréquence des mots par fichier\n",
    "corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "\n",
    "# Format lisible d'un extrait du corpus\n",
    "[[(id2word[id], freq) for id, freq in cp[:10]] for cp in corpus[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 2   # Le nombre minimum de thèmes par modèle\n",
    "limit = 10  # Le nombre maximum de thèmes par modèle\n",
    "step = 2    # Le pas d'augmentation du nombre de thèmes\n",
    "multiple_num_topics = range(start, limit + 1, step)\n",
    "\n",
    "model_list = []\n",
    "coherence_values = []\n",
    "\n",
    "for num_topics in multiple_num_topics:\n",
    "    print(f'Avec {num_topics} thèmes...')\n",
    "\n",
    "    model = LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        num_topics=num_topics,\n",
    "        id2word=id2word,\n",
    "        workers=1)\n",
    "    model_list.append(model)\n",
    "\n",
    "    coherencemodel = CoherenceModel(\n",
    "        model=model,\n",
    "        texts=data_lemmatized,\n",
    "        dictionary=id2word,\n",
    "        coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "print('Terminé')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le graphique des valeurs de cohérence\n",
    "plt.plot(multiple_num_topics, coherence_values)\n",
    "\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les valeurs de cohérence\n",
    "for m, cv in zip(multiple_num_topics, coherence_values):\n",
    "    print(f'Pour un nombre de thèmes = {m:2d},',\n",
    "          f'on obtient une cohérence de {round(cv, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choissisez le modèle que vous croyez être le meilleur\n",
    "# Rappel - les indices commencent à 0 dans Python\n",
    "optimal_model = model_list[3]\n",
    "\n",
    "# Affichage des différents thèmes\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relancer le modèle avec le nombre exact de thèmes\n",
    "ldamallet = LdaMulticore(corpus=corpus, num_topics=8, id2word=id2word, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les thèmes retenus\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Afficher la cohérance\n",
    "coherence_model_ldamallet = CoherenceModel(\n",
    "    model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nScore de cohérence: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=df):\n",
    "    # Créer un nouveau DataFrame\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Extraire les thèmes principaux de chaque document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        # Obtenir le Dominant_Topic, le Perc_Contribution et les Topic_Keywords\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => thème principal\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                    pd.Series(\n",
    "                        [int(topic_num), round(prop_topic,4), topic_keywords]),\n",
    "                    ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    sent_topics_df.columns = [\n",
    "        'Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Ajouter les colonnes nom de fichier et contenu\n",
    "    contents = texts\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les résultats finaux\n",
    "df_topic_sents_keywords = format_topics_sentences(\n",
    "    ldamodel=ldamallet, corpus=corpus, texts=df)\n",
    "\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "    'Document number',\n",
    "    'Dominant_Topic',\n",
    "    'Topic_Perc_Contrib',\n",
    "    'Keywords',\n",
    "    'file_name',\n",
    "    'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats finaux\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 Kernel",
   "language": "python",
   "name": "20220211_1755"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
